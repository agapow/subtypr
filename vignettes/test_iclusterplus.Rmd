---
title: "Workflow of iClusterBayes"
output: html_notebook
---

Here are some tests on iClusterBayes that I did but I recommend to read part 4.1 in [iClusterBayes' article](https://academic.oup.com/biostatistics/article/19/1/71/3852318#106241193) along with iManual.pdf (access it with `iClusterPlus::iManual()` ) at page 15 that describe how the package is used.

This the standard procedure to use iClusterBayes. First, let's import iClusterPlus and some data (Glioblastoma multiform from TCGA repository)

```{r setup}
library(iClusterPlus)

library(subtypr)
library(CancerSubtypes)

data.list <- gbm.data[c("gene.exp","methy.exp", "mirna.exp")]
data.list <- lapply(data.list, t)
```

Then check the distribution of the data:

```{r data_check}
data.checkDistribution(t(data.list$gene.exp))
data.checkDistribution(t(data.list$mirna.exp))
data.checkDistribution(t(data.list$methy.exp))
```

The data is normalized.


Now tune the Bayesian model, trying several possible number of k (~ number of cluster - 1). This step is very computationaly intensive. 


## Basic tuning 

```{r tuning, eval=FALSE}
set.seed(777)
t0 <- Sys.time()
cv.fit <- tune.iClusterBayes(cpus = 7, dt1 = data.list[[1]],
                             dt2 = data.list[[2]],
                             dt3 = data.list[[3]],
                             type = c("gaussian", "gaussian","gaussian"),
                             K = 1:6, 
                             n.burnin = 1000, 
                             n.draw = 1200)
t_tot <- Sys.time() - t0

save(cv.fit, t_tot, file = "./tuning.RData")
                                                     
```

it took 18 minutes which is fast but we've not set a lot of iterations so let's see the acuracy of the results...

```{r getBICv2, include=FALSE}

# the built-in getBIC doesn't work for the format returned by tune.iClusterBayes so here is a custom version

getBICBayes <- function (cv.fit, k.list) {
  n.K <- length(k.list)
  BIC <- data.frame(K = k.list, BIC = k.list)
  for (i in 1:n.K) {
    BIC$BIC[i] <- cv.fit$fit[[i]]$BIC
  }
  BIC
}

getDevRBayes <- function(cv.fit, k.list) {
  n.K <- length(k.list)
  DevR <- data.frame(K = k.list, DevR = k.list)
  for (i in 1:n.K) {
    DevR$DevR[i] <- cv.fit$fit[[i]]$dev.ratio 
  }
  DevR
}

```

To choose the optimal number of cluster, we pick the value of k corresponding to the minimum BIC value

```{r BIC}
load("tuning.RData")
BIClist <- getBICBayes(cv.fit, 1:6)
plot(BIClist)
```

Here, the suggested number is K=1 meaning K+1 = 2 clusters...

We can also look for a plateau in the deviance ratio to find the optimal k:

```{r deviance_ratio}
DevR <- getDevRBayes(cv.fit, 1:6)
plot(DevR, ylab= "Deviance ratio")
```
 
 Here the plateau is reached in K=3, meaning 4 clusters as an optimal number...
 
 In the GBM, there is 3 established sub-types 
 
 Let's try anyway to analyze the results for K=3 and K=1
 
```{r heatmap_k_eq_3}
library(lattice) #need this to find the functions levelplot() and bluered()
library(gplots)
iClusterPlus::plotHMBayes(cv.fit$fit[[3]], data.list, type = c("gaussian", "gaussian", "gaussian"))

```
 
 HUm...
 
```{r heatmap_k_eq_2}
iClusterPlus::plotHMBayes(cv.fit$fit[[2]], data.list, type = c("gaussian", "gaussian", "gaussian"))
```
 
 It's also possible to make some feature selection using lasso coefficiemnt estimates.
 
 Let's try with K=3.
 
```{r features_selec}

best.fit <- cv.fit$fit[[3]]

features = alist()
features[[1]] = colnames(data.list[[1]])
features[[2]] = colnames(data.list[[2]])
features[[3]] = colnames(data.list[[3]])

sigfeatures=alist()

for(i in 1:3){
  rowsum=apply(abs(best.fit$beta[[i]]), 1, sum)
  upper=quantile(rowsum,prob=0.75)
  sigfeatures[[i]]=(features[[i]])[which(rowsum>upper)]
}

data.list.selected <- vector('list', 3)

for (i in 1:3) {
  data.list.selected[[i]] <- data.list[[i]][, sigfeatures[[i]]]
}

iClusterPlus::plotHMBayes(cv.fit$fit[[3]], data.list.selected, type = c("gaussian", "gaussian", "gaussian"))
```
 
 It improves the visualization of differences in mrna expression (gene.exp)
 
 We can seek for improvements. 
 
How to pre-process the data?
How to set the parameters for the tuning of the model in `tune.iClusterBayes` ? i.e. how to set  :

`n.burnin` : Number of MCMC burnin.

`n.draw`:	Number of MCMC draw.

`prior.gamma`: Prior probability for the indicator variable gamma of each data set.

`sdev`: Standard deviation of random walk proposal for the latent variable.

`beta.var.scale`: A positive value to control the scale of covariance matrix of the proposed beta.

`thin`: A parameter to thin the MCMC chain in order to reduce autocorrelation. Discard all but every 'thin'th sampling values. When thin=1, all sampling values are kept.

`pp.cutoff`: Posterior probability cutoff for the indicator variable gamma. The BIC and deviance ratio will be calculated by setting parameter beta to zero when the posterior probability of gamma <= cutoff.


## Better tuning and a different dataset.
 
```{r better_tuning}

```
 
 
